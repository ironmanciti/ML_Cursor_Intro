{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae55c84f",
   "metadata": {
    "id": "ae55c84f"
   },
   "source": [
    "# Autoregressive (자동회귀) 문장 생성 - HyperClova 모델\n",
    "\n",
    "HyperClova 모델을 사용한 자동 회귀적인 텍스트 생성 예제입니다.\n",
    "HyperClova는 네이버에서 개발한 한국어 특화 대화형 언어모델로,\n",
    "채팅 템플릿을 사용하여 자연스러운 대화형 텍스트를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b75689",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00b75689",
    "outputId": "4deb4f03-f146-4b65-e1ee-72713bffd0c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 텍스트: 옛날 옛적에\n",
      "토크나이저 vocab 크기: 110491\n",
      "특수 토큰들: {'bos_token': '<|endoftext|>', 'eos_token': '<|endofturn|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|endoftext|>', '<|fim_prefix|>', '<|fim_middle|>', '<|fim_suffix|>', '<|endofprompt|>', '<|_unuse_missing_100256|>', '<|_unuse_missing_100261|>', '<|_unuse_missing_100262|>', '<|_unuse_missing_100263|>', '<|_unuse_missing_100264|>', '<|_unuse_missing_100265|>', '<|_unuse_missing_100266|>', '<|_unuse_missing_100267|>', '<|_unuse_missing_100268|>', '<|_unuse_missing_100269|>', '<|_unuse_missing_100270|>', '<|_unuse_missing_100271|>', '<|im_start|>', '<|im_end|>', '<|stop|>', '<|endofturn|>', '<repo_name>', '<file_sep>', '<issue_start>', '<issue_comment>', '<issue_closed>', '<jupyter_start>', '<jupyter_text>', '<jupyter_code>', '<jupyter_output>', '<jupyter_script>', '<empty_output>', '<code_to_intermediate>', '<intermediate_to_code>', '<pr>', '<pr_status>', '<pr_is_merged>', '<pr_base>', '<pr_file>', '<pr_base_code>', '<pr_diff>', '<pr_diff_hunk>', '<pr_comment>', '<pr_event_id>', '<pr_review>', '<pr_review_state>', '<pr_review_comment>', '<pr_in_reply_to_review_id>', '<pr_in_reply_to_comment_id>', '<pr_diff_hunk_comment_line>', '<NAME>', '<EMAIL>', '<KEY>', '<PASSWORD>']}\n",
      "토큰화된 결과: tensor([[ 36092,    249, 104013, 102476,  82068,  19954]])\n",
      "토큰 개수: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 36092,    249, 104013, 102476,  82068,  19954]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# HyperClova 모델 및 토크나이저 초기화\n",
    "# HyperClova는 네이버에서 개발한 한국어 특화 대화형 언어모델\n",
    "model_name = \"naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 문장 시작 부분 (한국어로 변경)\n",
    "input_text = \"옛날 옛적에\"\n",
    "print(f\"원본 텍스트: {input_text}\")\n",
    "\n",
    "# 토크나이저 정보 확인\n",
    "print(f\"토크나이저 vocab 크기: {tokenizer.vocab_size}\")\n",
    "print(f\"특수 토큰들: {tokenizer.special_tokens_map}\")\n",
    "\n",
    "# 간단한 토큰화 테스트\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "print(f\"토큰화된 결과: {input_ids}\")\n",
    "print(f\"토큰 개수: {input_ids.shape[1]}\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92f576b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92f576b2",
    "outputId": "1fd75fa7-53ca-4a2a-b841-7f376e34f191"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 채팅 템플릿 구성 ===\n",
      "1. system: 당신은 창의적인 이야기 작가입니다. 주어진 프롬프트로 흥미로운 이야기를 써주세요.\n",
      "2. user: 옛날 옛적에로 시작하는 이야기를 써주세요.\n",
      "\n",
      "=== 템플릿 적용 결과 ===\n",
      "입력 키들: ['input_ids', 'attention_mask']\n",
      "input_ids 형태: torch.Size([1, 44])\n",
      "GPU로 이동 완료\n"
     ]
    }
   ],
   "source": [
    "# HyperClova 채팅 템플릿을 사용한 문장 생성\n",
    "# HyperClova는 채팅 형식의 템플릿을 사용하여 대화형 텍스트 생성\n",
    "\n",
    "# 채팅 형식으로 입력 구성\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"당신은 창의적인 이야기 작가입니다. 주어진 프롬프트로 흥미로운 이야기를 써주세요.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{input_text}로 시작하는 이야기를 써주세요.\"}\n",
    "]\n",
    "\n",
    "print(\"=== 채팅 템플릿 구성 ===\")\n",
    "for i, message in enumerate(chat):\n",
    "    print(f\"{i+1}. {message['role']}: {message['content']}\")\n",
    "\n",
    "# 채팅 템플릿 적용\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    chat,\n",
    "    add_generation_prompt=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(f\"\\n=== 템플릿 적용 결과 ===\")\n",
    "print(f\"입력 키들: {list(inputs.keys())}\")\n",
    "print(f\"input_ids 형태: {inputs['input_ids'].shape}\")\n",
    "\n",
    "# GPU 사용 가능하면 GPU로 이동\n",
    "if torch.cuda.is_available():\n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    print(\"GPU로 이동 완료\")\n",
    "else:\n",
    "    print(\"CPU 사용 중\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "967494d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "967494d6",
    "outputId": "63adc3d8-80db-4eab-bc67-1641978baebd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 생성된 텍스트 ===\n",
      "<|im_start|>system\n",
      "당신은 창의적인 이야기 작가입니다. 주어진 프롬프트로 흥미로운 이야기를 써주세요.<|im_end|>\n",
      "<|im_start|>user\n",
      "옛날 옛적에로 시작하는 이야기를 써주세요.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "어느 날, 한 마을의 작은 가게에서 일하는 소녀가 있었습니다.\n",
      "\n",
      "그녀는 매일 아침 일찍 일어나서 가게 문을 열고 손님들을 맞이했습니다. 그녀는 손님의 요구를 듣고 필요한 물건을 찾아주고, 친절하게 안내해주며, 항상 웃으며 일을 했습니다. 그녀의 이름은 '소피아'였습니다.\n",
      "\n",
      "하루는 소피아가 손님에게 물었습니다.\n",
      "\"안녕하세요! 오늘은 어떤 물건으로 오셨나요?\"\n",
      "손님은 고개를 끄덕이며 대답했습니다.\n",
      "\"저는 오늘 새로운 책을 사려고 합니다.\"\n",
      "소피아는 그 말을 듣고 기뻐하며 책장을 열어보았습니다. 그리고 그녀가 찾고 있던 책이 바로 그곳에 있다는 것을 깨달았습니다!\n",
      "책은 매우 아름답고, 내용이 풍부하고, 그림이 그려져 있어서 정말 마음에 들었습니다!\n",
      "\n",
      "\n",
      "\n",
      "=== 새로 생성된 부분만 ===\n",
      "어느 날, 한 마을의 작은 가게에서 일하는 소녀가 있었습니다.\n",
      "\n",
      "그녀는 매일 아침 일찍 일어나서 가게 문을 열고 손님들을 맞이했습니다. 그녀는 손님의 요구를 듣고 필요한 물건을 찾아주고, 친절하게 안내해주며, 항상 웃으며 일을 했습니다. 그녀의 이름은 '소피아'였습니다.\n",
      "\n",
      "하루는 소피아가 손님에게 물었습니다.\n",
      "\"안녕하세요! 오늘은 어떤 물건으로 오셨나요?\"\n",
      "손님은 고개를 끄덕이며 대답했습니다.\n",
      "\"저는 오늘 새로운 책을 사려고 합니다.\"\n",
      "소피아는 그 말을 듣고 기뻐하며 책장을 열어보았습니다. 그리고 그녀가 찾고 있던 책이 바로 그곳에 있다는 것을 깨달았습니다!\n",
      "책은 매우 아름답고, 내용이 풍부하고, 그림이 그려져 있어서 정말 마음에 들었습니다!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HyperClova 텍스트 생성\n",
    "# generate 메서드의 주요 파라미터들:\n",
    "# - max_length: 생성할 최대 토큰 수\n",
    "# - stop_strings: 생성 중단 문자열들\n",
    "# - repetition_penalty: 반복 방지 페널티 (1.0 이상 권장)\n",
    "# - temperature: 생성 다양성 조절 (기본값 사용)\n",
    "\n",
    "output_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_length=200,\n",
    "    stop_strings=[\"<|endofturn|>\", \"<|stop|>\"],\n",
    "    repetition_penalty=1.2,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "generated_text = tokenizer.batch_decode(output_ids)[0]\n",
    "print(\"=== 생성된 텍스트 ===\")\n",
    "print(generated_text)\n",
    "\n",
    "# 원본 입력과 새로 생성된 부분 분리\n",
    "input_length = inputs['input_ids'].shape[1]\n",
    "new_tokens = output_ids[0][input_length:]\n",
    "new_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "print(f\"\\n=== 새로 생성된 부분만 ===\")\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10e2ff5",
   "metadata": {
    "id": "c10e2ff5"
   },
   "source": [
    "HyperClova는 자체적으로 autoregressive 모델입니다. \"Autoregressive\"란, 이전에 생성된 토큰들을 기반으로 다음 토큰을 생성하는 모델을 의미합니다.\n",
    "\n",
    "위의 코드에서 `model.generate` 메서드는 이미 autoregressive한 방식으로 문장을 생성합니다. 그러나 이를 명시적으로 보여주기 위해 각 단계에서 토큰을 하나씩 생성하는 autoregressive한 코드를 아래에 작성하겠습니다:\n",
    "\n",
    "## 다양한 한국어 프롬프트로 실험해보기\n",
    "\n",
    "다음 셀에서 다양한 한국어 프롬프트로 텍스트 생성을 시도해볼 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b3468a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b3468a9",
    "outputId": "c26a6f17-15fc-463e-dfc5-6a6b0ac45a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 다양한 한국어 프롬프트로 텍스트 생성 실험 ===\n",
      "\n",
      "1. 프롬프트: '인공지능의 미래는'\n",
      "\n",
      "   생성된 텍스트: **AI 기술의 발전과 그 영향**\n",
      "\n",
      "1. **기계 학습 및 딥러닝**: 인공지능은 기계 학습(ML)과 딥 러닝(DL)을 기반으로 합니다. 이러한 기술은 데이터로부터 패턴을 인식하고 예측하는 능력을 향상시켜, 다양한 분야에서 혁신적 변화를 가져오고 있습니다.\n",
      "\n",
      "2. **자연어 처리(NLP)**: NLP 기술을 통해 컴퓨터가 인간의 언어를 이해하고 생성할 수 있게 되었습니다. 이는 음성 인식부터 번역까지 광범위한 응용 분야로 확장되고 있으며, 의료 진단이나 고객 서비스에도 활용됩니다\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "2. 프롬프트: '기술이 발달한 세상에서'\n",
      "\n",
      "   생성된 텍스트: **1. 인공지능(AI)의 발전**\n",
      "\n",
      "인공지능은 컴퓨터가 인간의 지능을 모방하여 학습하고 문제를 해결하는 기술로, 최근 몇 년간 급속도로 발전했습니다.\n",
      "\n",
      "- **기계학습**: 머신러닝과 딥러닝 기술을 사용하여 데이터를 분석하고 패턴을 인식합니다.\n",
      "  - 예: 이미지 인식 시스템 (예: 얼굴인식), 자연어 처리(NLP)\n",
      "  \n",
      "- **강화학습(Deep Learning)**:\n",
      "  - 신경망 구조를 기반으로 한 모델로서 복잡한 데이터도 효과적으로 학습할 수 있습니다\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "3. 프롬프트: '인생에서 가장 중요한 것은'\n",
      "\n",
      "   생성된 텍스트: 저는 인공지능으로서 개인적인 의견을 가질 수 없습니다만, 인생의 중요성에 대한 일반적인 견해를 말씀드릴 수는 있습니다.\n",
      "\n",
      "1. **행복**: 행복을 추구하는 것이 중요합니다. 이는 개인의 삶의 질을 높이고 만족감을 주는 요소로 작용합니다.\n",
      "2. **관계 형성**: 가족이나 친구와의 관계를 유지하는 것도 매우 중요합니다. 인간은 사회적 동물이기 때문에 관계는 우리의 삶에 큰 영향을 미칩니다.\n",
      "3. **배움**: 새로운 지식과 기술을 배우는 것 또한 중요합니다. 배움을 통해 우리는 성장하고 발전할 수 있으며, 더 나은\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 다양한 한국어 프롬프트로 텍스트 생성 실험\n",
    "prompts = [\n",
    "    \"인공지능의 미래는\",\n",
    "    \"기술이 발달한 세상에서\",\n",
    "    \"인생에서 가장 중요한 것은\"\n",
    "]\n",
    "\n",
    "print(\"=== 다양한 한국어 프롬프트로 텍스트 생성 실험 ===\\n\")\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"{i}. 프롬프트: '{prompt}'\")\n",
    "\n",
    "    # 채팅 형식으로 구성\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": \"당신은 창의적이고 지적인 AI 어시스턴트입니다.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{prompt}에 대해 자세히 설명해주세요.\"}\n",
    "    ]\n",
    "\n",
    "    # 템플릿 적용\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        add_generation_prompt=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # GPU 사용 가능하면 GPU로 이동\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "    # 텍스트 생성\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=150,\n",
    "        stop_strings=[\"<|endofturn|>\", \"<|stop|>\"],\n",
    "        repetition_penalty=1.2,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    # 결과 출력\n",
    "    generated_text = tokenizer.batch_decode(output_ids)[0]\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    new_text = tokenizer.decode(output_ids[0][input_length:], skip_special_tokens=True)\n",
    "    print(f\"\\n   생성된 텍스트: {new_text}\")\n",
    "    print(\"-\" * 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "573d7029",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "573d7029",
    "outputId": "ecbfae50-512b-4ff3-fdf1-c4fd16b221f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 프롬프트: 옛날 옛적에\n",
      "템플릿 적용 후 토큰 수: 31\n",
      "토큰화된 결과: tensor([[100272,   9125,    198,  65895, 100743, 104848, 101088,  80052,     13,\n",
      "         100273,    198, 100272,    882,    198,  36092,    249, 104013, 102476,\n",
      "          82068,  19954,  17835, 101208, 105077, 104144,  92769,     13, 100273,\n",
      "            198, 100272,  78191,    198]])\n"
     ]
    }
   ],
   "source": [
    "# HyperClova Autoregressive 생성 과정 분석\n",
    "# 채팅 템플릿을 사용한 입력 구성\n",
    "input_text = \"옛날 옛적에\"\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"당신은 이야기 작가입니다.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{input_text}로 시작하는 이야기를 써주세요.\"}\n",
    "]\n",
    "\n",
    "# 템플릿 적용\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    chat,\n",
    "    add_generation_prompt=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(f\"원본 프롬프트: {input_text}\")\n",
    "print(f\"템플릿 적용 후 토큰 수: {inputs['input_ids'].shape[1]}\")\n",
    "print(f\"토큰화된 결과: {inputs['input_ids']}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d0817b9-4a08-45b2-90a3-1076b6284057",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9d0817b9-4a08-45b2-90a3-1076b6284057",
    "outputId": "238e575d-310f-48ff-d832-bacda1a3ac84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 입력 토큰 수: 31\n",
      "=== HyperClova Autoregressive 생성 과정 ===\n",
      "\n",
      "시작 프롬프트: '옛날 옛적에'\n",
      "\n",
      "목표 길이: 61 토큰\n",
      "\n",
      "         Assistant 응답: '�\n",
      "         Assistant 응답: '옛\n",
      "         Assistant 응답: '옛날\n",
      "         Assistant 응답: '옛날 옛\n",
      "         Assistant 응답: '옛날 옛적\n",
      "         Assistant 응답: '옛날 옛적에\n",
      "         Assistant 응답: '옛날 옛적에,\n",
      "         Assistant 응답: '옛날 옛적에, 한\n",
      "         Assistant 응답: '옛날 옛적에, 한 마을\n",
      "         Assistant 응답: '옛날 옛적에, 한 마을에\n",
      "         Assistant 응답: '옛날 옛적에, 한 마을에 한\n",
      "         Assistant 응답: '옛날 옛적에, 한 마을에 한 소녀\n",
      "         Assistant 응답: '옛날 옛적에, 한 마을에 한 소녀가\n",
      "         Assistant 응답: '옛날 옛적에, 한 마을에 한 소녀가 살고\n",
      "         Assistant 응답: '옛날 옛적에, 한 마을에 한 소녀가 살고 있\n",
      "         Assistant 응답: '옛날 옛적에, 한 마을에 한 소녀가 살고 있�\n",
      "         Assistant 응답: '옛날 옛적에, 한 마을에 한 소녀가 살고 있었습니다\n",
      "         Assistant 응답: '옛날 옛적에, 한 마을에 한 소녀가 살고 있었습니다.\n",
      "         Assistant 응답: '옛날 옛적에, 한 마을에 한 소녀가 살고 있었습니다. 그녀의\n",
      "         Assistant 응답: '옛날 옛적에, 한 마을에 한 소녀가 살고 있었습니다. 그녀의 이름은\n",
      "         Assistant 응답: '옛날 옛적에, 한 마을에 한 소녀가 살고 있었습니다. 그녀의 이름은 '\n",
      "         Assistant 응답: '옛날 옛적에, 한 마을에 한 소녀가 살고 있었습니다. 그녀의 이름은 '소\n",
      "         Assistant 응답: '옛날 옛적에, 한 마을에 한 소녀가 살고 있었습니다. 그녀의 이름은 '소녀\n",
      "         Assistant 응답: '옛날 옛적에, 한 마을에 한 소녀가 살고 있었습니다. 그녀의 이름은 '소녀의\n",
      "         Assistant 응답: '옛날 옛적에, 한 마을에 한 소녀가 살고 있었습니다. 그녀의 이름은 '소녀의 마음\n",
      "         Assistant 응답: '옛날 옛적에, 한 마을에 한 소녀가 살고 있었습니다. 그녀의 이름은 '소녀의 마음'\n",
      "         Assistant 응답: '옛날 옛적에, 한 마을에 한 소녀가 살고 있었습니다. 그녀의 이름은 '소녀의 마음'이\n",
      "         Assistant 응답: '옛날 옛적에, 한 마을에 한 소녀가 살고 있었습니다. 그녀의 이름은 '소녀의 마음'이�\n",
      "         Assistant 응답: '옛날 옛적에, 한 마을에 한 소녀가 살고 있었습니다. 그녀의 이름은 '소녀의 마음'이었습니다\n",
      "         Assistant 응답: '옛날 옛적에, 한 마을에 한 소녀가 살고 있었습니다. 그녀의 이름은 '소녀의 마음'이었습니다.\n",
      "생성 완료!\n"
     ]
    }
   ],
   "source": [
    "# HyperClova Autoregressive한 방식으로 문장 생성\n",
    "# 각 단계에서 이전 토큰들을 기반으로 다음 토큰을 예측하는 과정을 시각화\n",
    "\n",
    "# 현재 입력 토큰 수 확인\n",
    "current_length = inputs['input_ids'].shape[1]\n",
    "print(f\"현재 입력 토큰 수: {current_length}\")\n",
    "\n",
    "max_length = current_length + 30  # 현재 길이에서 20개 토큰 추가 생성\n",
    "input_ids_concat = inputs['input_ids'].clone()\n",
    "\n",
    "print(\"=== HyperClova Autoregressive 생성 과정 ===\\n\")\n",
    "print(f\"시작 프롬프트: '{input_text}'\\n\")\n",
    "print(f\"목표 길이: {max_length} 토큰\\n\")\n",
    "\n",
    "# 원본 입력 길이 저장 (assistant 응답 부분만 추출하기 위해)\n",
    "original_length = current_length\n",
    "\n",
    "step = 0\n",
    "while input_ids_concat.shape[1] < max_length:\n",
    "    step += 1\n",
    "\n",
    "    # 다음 토큰 예측\n",
    "    model_inputs = {\"input_ids\": input_ids_concat}\n",
    "    if \"attention_mask\" in inputs:\n",
    "        model_inputs[\"attention_mask\"] = torch.ones_like(input_ids_concat)\n",
    "\n",
    "    predictions = model(**model_inputs)\n",
    "    logits = predictions.logits\n",
    "    predicted_token = torch.argmax(logits[0, -1]).item()\n",
    "\n",
    "    # 생성된 토큰을 입력 토큰 뒤에 추가 (같은 장치에 맞춤)\n",
    "    new_token_tensor = torch.tensor([[predicted_token]], device=input_ids_concat.device)\n",
    "    input_ids_concat = torch.cat([input_ids_concat, new_token_tensor], dim=1)\n",
    "\n",
    "    # assistant가 생성한 부분만 추출\n",
    "    assistant_tokens = input_ids_concat[0][original_length:]\n",
    "    assistant_text = tokenizer.decode(assistant_tokens, skip_special_tokens=True)\n",
    "    new_token = tokenizer.decode(predicted_token, skip_special_tokens=True)\n",
    "    print(f\"         Assistant 응답: '{assistant_text}\")\n",
    "\n",
    "    # 너무 길어지면 중단\n",
    "    if step > 30:\n",
    "        break\n",
    "\n",
    "print(\"생성 완료!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

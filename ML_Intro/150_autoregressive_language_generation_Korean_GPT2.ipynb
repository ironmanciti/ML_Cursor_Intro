{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae55c84f",
   "metadata": {
    "id": "ae55c84f"
   },
   "source": [
    "# Autoregressive (자동회귀) 문장 생성 - HyperClova 모델\n",
    "\n",
    "HyperClova 모델을 사용한 자동 회귀적인 텍스트 생성 예제입니다.\n",
    "HyperClova는 네이버에서 개발한 한국어 특화 대화형 언어모델로,\n",
    "채팅 템플릿을 사용하여 자연스러운 대화형 텍스트를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00b75689",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00b75689",
    "outputId": "4deb4f03-f146-4b65-e1ee-72713bffd0c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyperCLOVAX 모델에 토크나이저 문제가 있어서 대안 모델을 사용합니다.\n",
      "SKT KoGPT-2 모델로 전환합니다...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f85f8550dc416b8e84420f868c026d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trimu\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\trimu\\.cache\\huggingface\\hub\\models--skt--kogpt2-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d31a8392cc4d31a21a8b134bc63c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토크나이저 로드 성공: skt/kogpt2-base-v2\n",
      "패딩 토큰을 EOS 토큰으로 설정했습니다.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a45ed7f1254c4289a08ed19838f91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trimu\\anaconda3\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 로드 성공: skt/kogpt2-base-v2\n",
      "CPU를 사용합니다.\n",
      "원본 텍스트: 옛날 옛적에\n",
      "토크나이저 vocab 크기: 51200\n",
      "특수 토큰들: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}\n",
      "토큰화된 결과: tensor([[12346, 10256, 16871]])\n",
      "토큰 개수: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[12346, 10256, 16871]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# HyperCLOVAX 모델에 문제가 있어서 안정적인 대안 모델 사용\n",
    "# SKT에서 개발한 한국어 GPT-2 모델로 변경\n",
    "print(\"HyperCLOVAX 모델에 토크나이저 문제가 있어서 대안 모델을 사용합니다.\")\n",
    "print(\"SKT KoGPT-2 모델로 전환합니다...\\n\")\n",
    "\n",
    "# 안정적인 한국어 모델 사용\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"토크나이저 로드 성공: {model_name}\")\n",
    "\n",
    "# 패딩 토큰 설정 (KoGPT-2는 기본적으로 패딩 토큰이 없음)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"패딩 토큰을 EOS 토큰으로 설정했습니다.\")\n",
    "\n",
    "# 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=None,\n",
    "    low_cpu_mem_usage=False\n",
    ")\n",
    "print(f\"모델 로드 성공: {model_name}\")\n",
    "\n",
    "# GPU 사용 가능하면 GPU로 이동\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")\n",
    "    print(\"모델을 GPU로 이동했습니다.\")\n",
    "else:\n",
    "    print(\"CPU를 사용합니다.\")\n",
    "\n",
    "# 문장 시작 부분 (한국어로 변경)\n",
    "input_text = \"옛날 옛적에\"\n",
    "print(f\"원본 텍스트: {input_text}\")\n",
    "\n",
    "# 토크나이저 정보 확인\n",
    "print(f\"토크나이저 vocab 크기: {tokenizer.vocab_size}\")\n",
    "print(f\"특수 토큰들: {tokenizer.special_tokens_map}\")\n",
    "\n",
    "# 간단한 토큰화 테스트\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "print(f\"토큰화된 결과: {input_ids}\")\n",
    "print(f\"토큰 개수: {input_ids.shape[1]}\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3181a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대안 모델 옵션들:\n",
      "1. skt/kogpt2-base-v2 - SKT에서 개발한 한국어 GPT-2\n",
      "2. microsoft/DialoGPT-medium - 다국어 대화형 모델\n",
      "3. facebook/blenderbot-400M-distill - 대화형 모델\n",
      "\n",
      "HyperCLOVAX 모델에 문제가 있다면 위 모델 중 하나를 사용하세요.\n"
     ]
    }
   ],
   "source": [
    "# 대안: 다른 한국어 모델 사용 (HyperCLOVAX에 문제가 있을 경우)\n",
    "# 이 셀은 HyperCLOVAX 모델에 문제가 있을 때만 실행하세요\n",
    "\n",
    "# 대안 1: 한국어 지원 GPT-2 모델\n",
    "# model_name_alt = \"skt/kogpt2-base-v2\"\n",
    "\n",
    "# 대안 2: 한국어 BERT 기반 모델 (생성용으로는 제한적)\n",
    "# model_name_alt = \"klue/bert-base\"\n",
    "\n",
    "# 대안 3: 다국어 지원 모델\n",
    "# model_name_alt = \"microsoft/DialoGPT-medium\"\n",
    "\n",
    "print(\"대안 모델 옵션들:\")\n",
    "print(\"1. skt/kogpt2-base-v2 - SKT에서 개발한 한국어 GPT-2\")\n",
    "print(\"2. microsoft/DialoGPT-medium - 다국어 대화형 모델\")\n",
    "print(\"3. facebook/blenderbot-400M-distill - 대화형 모델\")\n",
    "print(\"\\nHyperCLOVAX 모델에 문제가 있다면 위 모델 중 하나를 사용하세요.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92f576b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92f576b2",
    "outputId": "1fd75fa7-53ca-4a2a-b841-7f376e34f191"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== KoGPT-2 텍스트 생성 ===\n",
      "입력 텍스트: '옛날 옛적에'\n",
      "토큰화된 결과: tensor([[12346, 10256, 16871]])\n",
      "토큰 개수: 3\n",
      "CPU를 사용합니다.\n"
     ]
    }
   ],
   "source": [
    "# KoGPT-2를 사용한 한국어 텍스트 생성\n",
    "# KoGPT-2는 일반적인 텍스트 생성 방식 사용 (채팅 템플릿 없음)\n",
    "\n",
    "print(\"=== KoGPT-2 텍스트 생성 ===\")\n",
    "print(f\"입력 텍스트: '{input_text}'\")\n",
    "\n",
    "# 입력 텍스트를 토큰화\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "print(f\"토큰화된 결과: {input_ids}\")\n",
    "print(f\"토큰 개수: {input_ids.shape[1]}\")\n",
    "\n",
    "# GPU 사용 가능하면 GPU로 이동\n",
    "if torch.cuda.is_available():\n",
    "    input_ids = input_ids.to(\"cuda\")\n",
    "    print(\"입력을 GPU로 이동했습니다.\")\n",
    "else:\n",
    "    print(\"CPU를 사용합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "967494d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "967494d6",
    "outputId": "63adc3d8-80db-4eab-bc67-1641978baebd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== KoGPT-2 텍스트 생성 시작 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 생성된 전체 텍스트 ===\n",
      "옛날 옛적에야 우리도 우리랑 같이 놀 수 있게 되었으니까.\"\n",
      "\"어쨌든 우리한테는 정말 미안합니다.\"\n",
      "나는 그제서야 내 어깨를 감싸 안고 고개를 끄덕였다.\n",
      "그러자 갑자기 뭔가가 내 가슴에 와 닿았다.\n",
      "아직 한참이 지난 후였다.\n",
      "내가 아는 사이인지라 내 가슴속에는 아직도 그날의 기억들이 고스란히 남아 있었다.\n",
      "하지만 이토록 오랫동안 내가 잊고 지냈던 것은 그때만 해도 내게 있어 언제나 그런 것이었다.\n",
      "그날 이후로 나는 내 삶의 방식을 바꾸어놓으려고 노력\n",
      "\n",
      "=== 새로 생성된 부분만 ===\n",
      "야 우리도 우리랑 같이 놀 수 있게 되었으니까.\"\n",
      "\"어쨌든 우리한테는 정말 미안합니다.\"\n",
      "나는 그제서야 내 어깨를 감싸 안고 고개를 끄덕였다.\n",
      "그러자 갑자기 뭔가가 내 가슴에 와 닿았다.\n",
      "아직 한참이 지난 후였다.\n",
      "내가 아는 사이인지라 내 가슴속에는 아직도 그날의 기억들이 고스란히 남아 있었다.\n",
      "하지만 이토록 오랫동안 내가 잊고 지냈던 것은 그때만 해도 내게 있어 언제나 그런 것이었다.\n",
      "그날 이후로 나는 내 삶의 방식을 바꾸어놓으려고 노력\n"
     ]
    }
   ],
   "source": [
    "# KoGPT-2 텍스트 생성\n",
    "# generate 메서드의 주요 파라미터들:\n",
    "# - max_length: 생성할 최대 토큰 수\n",
    "# - num_return_sequences: 생성할 시퀀스 수\n",
    "# - temperature: 생성 다양성 조절 (높을수록 다양함)\n",
    "# - top_p: nucleus sampling 파라미터\n",
    "# - do_sample: 샘플링 사용 여부\n",
    "# - pad_token_id: 패딩 토큰 ID\n",
    "\n",
    "print(\"=== KoGPT-2 텍스트 생성 시작 ===\")\n",
    "\n",
    "# 텍스트 생성\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=input_ids.shape[1] + 100,  # 원본 길이 + 100 토큰\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"=== 생성된 전체 텍스트 ===\")\n",
    "print(generated_text)\n",
    "\n",
    "# 원본 입력과 새로 생성된 부분 분리\n",
    "input_length = input_ids.shape[1]\n",
    "new_tokens = output_ids[0][input_length:]\n",
    "new_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "print(f\"\\n=== 새로 생성된 부분만 ===\")\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10e2ff5",
   "metadata": {
    "id": "c10e2ff5"
   },
   "source": [
    "HyperClova는 자체적으로 autoregressive 모델입니다. \"Autoregressive\"란, 이전에 생성된 토큰들을 기반으로 다음 토큰을 생성하는 모델을 의미합니다.\n",
    "\n",
    "위의 코드에서 `model.generate` 메서드는 이미 autoregressive한 방식으로 문장을 생성합니다. 그러나 이를 명시적으로 보여주기 위해 각 단계에서 토큰을 하나씩 생성하는 autoregressive한 코드를 아래에 작성하겠습니다:\n",
    "\n",
    "## 다양한 한국어 프롬프트로 실험해보기\n",
    "\n",
    "다음 셀에서 다양한 한국어 프롬프트로 텍스트 생성을 시도해볼 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b3468a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b3468a9",
    "outputId": "c26a6f17-15fc-463e-dfc5-6a6b0ac45a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 다양한 한국어 프롬프트로 KoGPT-2 텍스트 생성 실험 ===\n",
      "\n",
      "1. 프롬프트: '인공지능의 미래는'\n",
      "\n",
      "   생성된 텍스트: ‘알파’다.\n",
      "최근 스마트폰에 적용된 알고리즘이 그 답을 알려줬다는 점에서 이 기술은 이미 기술혁신을 통한 산업발전의 기반이 되고 있다.\n",
      "구글, 애플 등 세계 주요 기업들은 이미 데이터 기반 플랫폼을 제공하고 있고 삼성전자, SK하이닉스, LG전자 등도 관련 기술을 준비하고 있는 것으로 알려졌다.\n",
      "그러나 아직까지는 상용화가 이뤄지지 않아 어떤 기업이 시장을\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "2. 프롬프트: '기술이 발달한 세상에서'\n",
      "\n",
      "   생성된 텍스트: 이 두 가지 방법은 모두 다 필요하지만 그것만으로는 부족합니다.\n",
      "그렇다면 어떻게 해야 합니다.\n",
      "우리가 알고 있는 것이 가장 확실하고 믿을 수 있습니다.\n",
      "바로 그 방법을 택해야 한다는 것입니다.\n",
      "그러나 모든 사람이 이러한 원칙을 알고 있을 때 가능하지 않을까요?\n",
      "아니면 우리가 알지 못하는 것을 알면서도 그것을 믿으려고 하지 말라는 법은 없을까요?\n",
      "결국 우리는 어떤 방법으로든지 우리를 이해하고 설득할 수밖에 없습니다.\n",
      "설득\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "3. 프롬프트: '인생에서 가장 중요한 것은'\n",
      "\n",
      "   생성된 텍스트: 바로 자기 자신을 사랑하고 자신감을 유지하는 것이다.\n",
      "그것은 우리 자신이 갖고 있는 모든 것을 소중하게 여기고 다른 사람과의 관계를 중요하게 여기는 삶이다.\n",
      "자신이 가지고 있던 모든 것에 대해 자부심을 가질 때 진정한 사랑과 행복이 오간다.\n",
      "자신을 사랑하는 것만으로는 부족하다.\n",
      "내가 가지고 있는 모든 것, 즉 자신의 소중한 것들을 타인에게 자랑하지 마라.\n",
      "아름다운 사람, 아름다운 사람은 남을 위해 기꺼이 헌신할 줄 아는 사람만이 남들보다\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 다양한 한국어 프롬프트로 KoGPT-2 텍스트 생성 실험\n",
    "prompts = [\n",
    "    \"인공지능의 미래는\",\n",
    "    \"기술이 발달한 세상에서\",\n",
    "    \"인생에서 가장 중요한 것은\"\n",
    "]\n",
    "\n",
    "print(\"=== 다양한 한국어 프롬프트로 KoGPT-2 텍스트 생성 실험 ===\\n\")\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"{i}. 프롬프트: '{prompt}'\")\n",
    "\n",
    "    # 입력 텍스트를 토큰화\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # GPU 사용 가능하면 GPU로 이동\n",
    "    if torch.cuda.is_available():\n",
    "        input_ids = input_ids.to(\"cuda\")\n",
    "\n",
    "    # 텍스트 생성\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=input_ids.shape[1] + 80,  # 원본 길이 + 80 토큰\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.8,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.2\n",
    "    )\n",
    "\n",
    "    # 결과 출력\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    input_length = input_ids.shape[1]\n",
    "    new_text = tokenizer.decode(output_ids[0][input_length:], skip_special_tokens=True)\n",
    "    print(f\"\\n   생성된 텍스트: {new_text}\")\n",
    "    print(\"-\" * 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "573d7029",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "573d7029",
    "outputId": "ecbfae50-512b-4ff3-fdf1-c4fd16b221f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 프롬프트: 옛날 옛적에\n",
      "토큰화 후 토큰 수: 3\n",
      "토큰화된 결과: tensor([[12346, 10256, 16871]])\n"
     ]
    }
   ],
   "source": [
    "# KoGPT-2 Autoregressive 생성 과정 분석\n",
    "# 간단한 입력 텍스트로 시작\n",
    "input_text = \"옛날 옛적에\"\n",
    "\n",
    "# 입력 텍스트를 토큰화\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"원본 프롬프트: {input_text}\")\n",
    "print(f\"토큰화 후 토큰 수: {input_ids.shape[1]}\")\n",
    "print(f\"토큰화된 결과: {input_ids}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    input_ids = input_ids.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d0817b9-4a08-45b2-90a3-1076b6284057",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9d0817b9-4a08-45b2-90a3-1076b6284057",
    "outputId": "238e575d-310f-48ff-d832-bacda1a3ac84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 입력 토큰 수: 3\n",
      "=== KoGPT-2 Autoregressive 생성 과정 ===\n",
      "\n",
      "시작 프롬프트: '옛날 옛적에'\n",
      "\n",
      "목표 길이: 33 토큰\n",
      "\n",
      "Step  1: 새로 생성된 텍스트: '그'\n",
      "Step  2: 새로 생성된 텍스트: '그 유명한'\n",
      "Step  3: 새로 생성된 텍스트: '그 유명한 ''\n",
      "Step  4: 새로 생성된 텍스트: '그 유명한 '청'\n",
      "Step  5: 새로 생성된 텍스트: '그 유명한 '청자'\n",
      "Step  6: 새로 생성된 텍스트: '그 유명한 '청자상감'\n",
      "Step  7: 새로 생성된 텍스트: '그 유명한 '청자상감운'\n",
      "Step  8: 새로 생성된 텍스트: '그 유명한 '청자상감운학문'\n",
      "Step  9: 새로 생성된 텍스트: '그 유명한 '청자상감운학문매'\n",
      "Step 10: 새로 생성된 텍스트: '그 유명한 '청자상감운학문매병'\n",
      "Step 11: 새로 생성된 텍스트: '그 유명한 '청자상감운학문매병'이'\n",
      "Step 12: 새로 생성된 텍스트: '그 유명한 '청자상감운학문매병'이 있었다.\n",
      "'\n",
      "Step 13: 새로 생성된 텍스트: '그 유명한 '청자상감운학문매병'이 있었다.\n",
      "그'\n",
      "Step 14: 새로 생성된 텍스트: '그 유명한 '청자상감운학문매병'이 있었다.\n",
      "그것은'\n",
      "Step 15: 새로 생성된 텍스트: '그 유명한 '청자상감운학문매병'이 있었다.\n",
      "그것은 ''\n",
      "Step 16: 새로 생성된 텍스트: '그 유명한 '청자상감운학문매병'이 있었다.\n",
      "그것은 '청'\n",
      "Step 17: 새로 생성된 텍스트: '그 유명한 '청자상감운학문매병'이 있었다.\n",
      "그것은 '청자'\n",
      "Step 18: 새로 생성된 텍스트: '그 유명한 '청자상감운학문매병'이 있었다.\n",
      "그것은 '청자상감'\n",
      "Step 19: 새로 생성된 텍스트: '그 유명한 '청자상감운학문매병'이 있었다.\n",
      "그것은 '청자상감운'\n",
      "Step 20: 새로 생성된 텍스트: '그 유명한 '청자상감운학문매병'이 있었다.\n",
      "그것은 '청자상감운학문'\n",
      "Step 21: 새로 생성된 텍스트: '그 유명한 '청자상감운학문매병'이 있었다.\n",
      "그것은 '청자상감운학문매'\n",
      "Step 22: 새로 생성된 텍스트: '그 유명한 '청자상감운학문매병'이 있었다.\n",
      "그것은 '청자상감운학문매병'\n",
      "Step 23: 새로 생성된 텍스트: '그 유명한 '청자상감운학문매병'이 있었다.\n",
      "그것은 '청자상감운학문매병'이'\n",
      "Step 24: 새로 생성된 텍스트: '그 유명한 '청자상감운학문매병'이 있었다.\n",
      "그것은 '청자상감운학문매병'이 아니라'\n",
      "Step 25: 새로 생성된 텍스트: '그 유명한 '청자상감운학문매병'이 있었다.\n",
      "그것은 '청자상감운학문매병'이 아니라 ''\n",
      "Step 26: 새로 생성된 텍스트: '그 유명한 '청자상감운학문매병'이 있었다.\n",
      "그것은 '청자상감운학문매병'이 아니라 '청'\n",
      "Step 27: 새로 생성된 텍스트: '그 유명한 '청자상감운학문매병'이 있었다.\n",
      "그것은 '청자상감운학문매병'이 아니라 '청자'\n",
      "Step 28: 새로 생성된 텍스트: '그 유명한 '청자상감운학문매병'이 있었다.\n",
      "그것은 '청자상감운학문매병'이 아니라 '청자상감'\n",
      "Step 29: 새로 생성된 텍스트: '그 유명한 '청자상감운학문매병'이 있었다.\n",
      "그것은 '청자상감운학문매병'이 아니라 '청자상감운'\n",
      "Step 30: 새로 생성된 텍스트: '그 유명한 '청자상감운학문매병'이 있었다.\n",
      "그것은 '청자상감운학문매병'이 아니라 '청자상감운학문'\n",
      "\n",
      "생성 완료!\n",
      "최종 생성된 텍스트: '옛날 옛적에 그 유명한 '청자상감운학문매병'이 있었다.\n",
      "그것은 '청자상감운학문매병'이 아니라 '청자상감운학문'\n"
     ]
    }
   ],
   "source": [
    "# KoGPT-2 Autoregressive한 방식으로 문장 생성\n",
    "# 각 단계에서 이전 토큰들을 기반으로 다음 토큰을 예측하는 과정을 시각화\n",
    "\n",
    "# 현재 입력 토큰 수 확인\n",
    "current_length = input_ids.shape[1]\n",
    "print(f\"현재 입력 토큰 수: {current_length}\")\n",
    "\n",
    "max_length = current_length + 30  # 현재 길이에서 30개 토큰 추가 생성\n",
    "input_ids_concat = input_ids.clone()\n",
    "\n",
    "print(\"=== KoGPT-2 Autoregressive 생성 과정 ===\\n\")\n",
    "print(f\"시작 프롬프트: '{input_text}'\\n\")\n",
    "print(f\"목표 길이: {max_length} 토큰\\n\")\n",
    "\n",
    "# 원본 입력 길이 저장 (새로 생성된 부분만 추출하기 위해)\n",
    "original_length = current_length\n",
    "\n",
    "step = 0\n",
    "while input_ids_concat.shape[1] < max_length:\n",
    "    step += 1\n",
    "\n",
    "    # 다음 토큰 예측\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_ids_concat)\n",
    "        logits = predictions.logits\n",
    "        predicted_token = torch.argmax(logits[0, -1]).item()\n",
    "\n",
    "    # 생성된 토큰을 입력 토큰 뒤에 추가 (같은 장치에 맞춤)\n",
    "    new_token_tensor = torch.tensor([[predicted_token]], device=input_ids_concat.device)\n",
    "    input_ids_concat = torch.cat([input_ids_concat, new_token_tensor], dim=1)\n",
    "\n",
    "    # 새로 생성된 부분만 추출\n",
    "    new_tokens = input_ids_concat[0][original_length:]\n",
    "    new_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    new_token = tokenizer.decode(predicted_token, skip_special_tokens=True)\n",
    "    print(f\"Step {step:2d}: 새로 생성된 텍스트: '{new_text}'\")\n",
    "\n",
    "    # 너무 길어지면 중단\n",
    "    if step > 30:\n",
    "        break\n",
    "\n",
    "print(\"\\n생성 완료!\")\n",
    "print(f\"최종 생성된 텍스트: '{tokenizer.decode(input_ids_concat[0], skip_special_tokens=True)}'\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
